---
title: 'Sentence-BERT'
lang: fr
subtitle: "Un puissant modèle de language pour l'encodage de phrases"
author:
  name: Francis Duval, Research Scientist | Co-operators
  email: francis_duval@cooperators.ca
date: 2024-11-16
date-format: long
format: 
  revealjs:
    template-partials:
      - title-slide.html
    theme: [default, styles.scss]
    width: 1280
    height: 720
    chalkboard: true
    transition: convex
    logo: images/logo_blue_2.png
    footer: '[https://github.com/francisduval/quarto_slides_cara_coop](https://github.com/francisduval/quarto_slides_cara_coop)'
    slide-number: c/t
    code-block-height: 500px
editor: 
  markdown: 
    wrap: 72
---

## Représentation vectorielle (embedding)

::: {.callout-note appearance="simple"}
## Représentation vectorielle
Une manière de convertir une chaine de caractères en **nombres** sous forme d'un **vecteur**.
:::

![](images/embeddings.jpg)

## Représentation vectorielle (embedding)

- Un vecteur seul n'a pas de signification.
- Ce sont les vecteurs pris ensemble qui ont du sens.

![](images/embeddings_2.png)


## Aperçu de Sentence-BERT


## HuggingFace

- A démocratisé l'utilisation des LLMs en rendant accessible leur utilisation et en fournissant une interface égale.



## Objectifs de la présentation

## Importance des représentations de phrases

## Limites des méthodes traditionnelles

## Sentence-BERT -- Inférence

## Sentence-BERT -- Entrainement

## Sentence-BERT -- Fine-tuning

## Application au problèmes de codes IBC

## Applications possibles

## HuggingFace


## Limites

- Complexité du fine-tuning et coût computationnel
- Pas idéal pour des textes très longs

## Pourquoi avons-nous besoin d'encoder des phrases?

## Notes

- Les LLM's sont des modèles qui permettent aux ordinateurs de « comprendre » du texte (alors qu'au fond, ils ne peuvent comprendre que des nombres)


